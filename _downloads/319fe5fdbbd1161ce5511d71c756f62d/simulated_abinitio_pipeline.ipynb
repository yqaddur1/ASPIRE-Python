{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Abinitio Pipeline - Simulated Data\n\nThis notebook introduces a selection of\ncomponents corresponding to generating realistic\nsimulated Cryo-EM data and running key ASPIRE-Python\nAbinitio model components as a pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\nFirst import some of the usual suspects.\nIn addition, import some classes from\nthe ASPIRE package that will be used throughout this experiment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom aspire.abinitio import CLSyncVoting\nfrom aspire.basis import FFBBasis2D, FFBBasis3D\nfrom aspire.classification import BFSReddyChatterjiAverager2D, RIRClass2D\nfrom aspire.denoising import DenoiserCov2D\nfrom aspire.noise import AnisotropicNoiseEstimator, CustomNoiseAdder\nfrom aspire.operators import FunctionFilter, RadialCTFFilter\nfrom aspire.reconstruction import MeanEstimator\nfrom aspire.source import Simulation\nfrom aspire.utils.coor_trans import (\n    get_aligned_rotations,\n    get_rots_mse,\n    register_rotations,\n)\nfrom aspire.volume import Volume\n\nlogger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters\nSome example simulation configurations.\nSmall sim: img_size 32, num_imgs 10000, n_classes 1000, n_nbor 10\nMedium sim: img_size 64, num_imgs 20000, n_classes 2000, n_nbor 10\nLarge sim: img_size 129, num_imgs 30000, n_classes 2000, n_nbor 20\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "interactive = True  # Draw blocking interactive plots?\ndo_cov2d = False  # Use CWF coefficients\nimg_size = 32  # Downsample the volume to a desired resolution\nnum_imgs = 10000  # How many images in our source.\nn_classes = 1000  # How many class averages to compute.\nn_nbor = 10  # How many neighbors to stack\nnoise_variance = 5e-7  # Set a target noise variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulation Data\nStart with a fairly hi-res volume available from EMPIAR/EMDB.\nhttps://www.ebi.ac.uk/emdb/EMD-2660\nhttps://ftp.ebi.ac.uk/pub/databases/emdb/structures/EMD-2660/map/emd_2660.map.gz\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "og_v = Volume.load(\"emd_2660.map\", dtype=np.float64)\nlogger.info(\"Original volume map data\" f\" shape: {og_v.shape} dtype:{og_v.dtype}\")\n\nlogger.info(f\"Downsampling to {(img_size,)*3}\")\nv = og_v.downsample(img_size)\nL = v.resolution\n\n\n# Then create a filter based on that variance\n# This is an example of a custom noise profile\ndef noise_function(x, y):\n    alpha = 1\n    beta = 1\n    # White\n    f1 = noise_variance\n    # Violet-ish\n    f2 = noise_variance * (x * x + y * y) / L * L\n    return (alpha * f1 + beta * f2) / 2.0\n\n\ncustom_noise = CustomNoiseAdder(noise_filter=FunctionFilter(noise_function))\n\nlogger.info(\"Initialize CTF filters.\")\n# Create some CTF effects\npixel_size = 5 * 65 / img_size  # Pixel size of the images (in angstroms)\nvoltage = 200  # Voltage (in KV)\ndefocus_min = 1.5e4  # Minimum defocus value (in angstroms)\ndefocus_max = 2.5e4  # Maximum defocus value (in angstroms)\ndefocus_ct = 7  # Number of defocus groups.\nCs = 2.0  # Spherical aberration\nalpha = 0.1  # Amplitude contrast\n\n# Create filters\nctf_filters = [\n    RadialCTFFilter(pixel_size, voltage, defocus=d, Cs=2.0, alpha=0.1)\n    for d in np.linspace(defocus_min, defocus_max, defocus_ct)\n]\n\n# Finally create the Simulation\nsrc = Simulation(\n    L=v.resolution,\n    n=num_imgs,\n    vols=v,\n    noise_adder=custom_noise,\n    unique_filters=ctf_filters,\n    dtype=v.dtype,\n)\n# Peek\nif interactive:\n    src.images[:10].show()\n\n# Use phase_flip to attempt correcting for CTF.\nlogger.info(\"Perform phase flip to input images.\")\nsrc.phase_flip()\n\n# Estimate the noise and `Whiten` based on the estimated noise\naiso_noise_estimator = AnisotropicNoiseEstimator(src)\nsrc.whiten(aiso_noise_estimator.filter)\n\n# Plot the noise profile for inspection\nif interactive:\n    plt.imshow(aiso_noise_estimator.filter.evaluate_grid(L))\n    plt.show()\n\n# Peek, what do the whitened images look like...\nif interactive:\n    src.images[:10].show()\n\n# Cache to memory for some speedup\nsrc.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: CWF Denoising\n\nOptionally generate an alternative source that is denoised with `cov2d`,\nthen configure a customized aligner. This allows the use of CWF denoised\nimages for classification, but stacks the original images for averages\nused in the remainder of the reconstruction pipeline.\n\nIn this example, this behavior is controlled by the `do_cov2d` boolean variable.\nWhen disabled, the original src and default aligner is used.\nIf you will not be using cov2d,\nyou may remove this code block and associated variables.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "classification_src = src\ncustom_averager = None\nif do_cov2d:\n    # Use CWF denoising\n    cwf_denoiser = DenoiserCov2D(src)\n    # Use denoised src for classification\n    classification_src = cwf_denoiser.denoise()\n    # Peek, what do the denoised images look like...\n    if interactive:\n        classification_src.images[:10].show()\n\n    # Use regular `src` for the alignment and composition (averaging).\n    composite_basis = FFBBasis2D((src.L,) * 2, dtype=src.dtype)\n    custom_averager = BFSReddyChatterjiAverager2D(composite_basis, src, dtype=src.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class Averaging\n\nNow perform classification and averaging for each class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "logger.info(\"Begin Class Averaging\")\n\nrir = RIRClass2D(\n    classification_src,  # Source used for classification\n    fspca_components=400,\n    bispectrum_components=300,  # Compressed Features after last PCA stage.\n    n_nbor=n_nbor,\n    n_classes=n_classes,\n    large_pca_implementation=\"legacy\",\n    nn_implementation=\"sklearn\",\n    bispectrum_implementation=\"legacy\",\n    averager=custom_averager,\n)\n\nclasses, reflections, distances = rir.classify()\navgs = rir.averages(classes, reflections, distances)\nif interactive:\n    avgs.images[:10].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Line Estimation\n\nNext create a CL instance for estimating orientation of projections\nusing the Common Line with Synchronization Voting method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "logger.info(\"Begin Orientation Estimation\")\n\n# Stash true rotations for later comparison,\n#   note this line only works with naive class selection...\ntrue_rotations = src.rotations[:n_classes]\n\norient_est = CLSyncVoting(avgs, n_theta=36)\n# Get the estimated rotations\norient_est.estimate_rotations()\nrots_est = orient_est.rotations\n\nlogger.info(\"Compare with known rotations\")\n# Compare with known true rotations\nQ_mat, flag = register_rotations(rots_est, true_rotations)\nregrot = get_aligned_rotations(rots_est, Q_mat, flag)\nmse_reg = get_rots_mse(regrot, true_rotations)\nlogger.info(\n    f\"MSE deviation of the estimated rotations using register_rotations : {mse_reg}\\n\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Volume Reconstruction\n\nUsing the estimated rotations, attempt to reconstruct a volume.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "logger.info(\"Begin Volume reconstruction\")\n\n# Assign the estimated rotations to the class averages\navgs.rotations = rots_est\n\n# Create a reasonable Basis for the 3d Volume\nbasis = FFBBasis3D((v.resolution,) * 3, dtype=v.dtype)\n\n# Setup an estimator to perform the back projection.\nestimator = MeanEstimator(avgs, basis)\n\n# Perform the estimation and save the volume.\nestimated_volume = estimator.estimate()\nfn = f\"estimated_volume_n{num_imgs}_c{n_classes}_m{n_nbor}_{img_size}.mrc\"\nestimated_volume.save(fn, overwrite=True)\n\n# Peek at result\nif interactive:\n    plt.imshow(np.sum(estimated_volume[0], axis=-1))\n    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}