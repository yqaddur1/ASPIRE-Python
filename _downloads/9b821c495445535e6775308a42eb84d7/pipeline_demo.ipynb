{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Ab-initio Pipeline Demonstration\n\nThis tutorial demonstrates some key components of an ab-initio reconstruction pipeline using\nsynthetic data generated with ASPIRE's ``Simulation`` class of objects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download a Volume\nWe begin by downloading a high resolution volume map of the 80S Ribosome, sourced from\nEMDB: https://www.ebi.ac.uk/emdb/EMD-2660.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nimport numpy as np\nimport requests\n\n\n# Download volume\ndef download(url, save_path, chunk_size=1024 * 1024):\n    r = requests.get(url, stream=True)\n    with open(save_path, \"wb\") as fd:\n        for chunk in r.iter_content(chunk_size=chunk_size):\n            fd.write(chunk)\n\n\nif not os.path.exists(\"data/emd_2660.map\"):\n    url = \"https://ftp.ebi.ac.uk/pub/databases/emdb/structures/EMD-2660/map/emd_2660.map.gz\"\n    download(url, \"data/emd_2660.map\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a Volume\nWe use ASPIRE's ``Volume`` class to load and downsample the volume.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from aspire.volume import Volume\n\n# Load 80s Ribosome\noriginal_vol = Volume.load(\"data/emd_2660.map\", dtype=np.float32)\n\n# Downsample the volume\nres = 41\nvol = original_vol.downsample(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>A ``Volume`` can be saved using the ``Volume.save()`` method as follows::\n\n        fn = f\"downsampled_80s_ribosome_size{res}.mrc\"\n        vol.save(fn, overwrite=True)</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Simulation Source\nASPIRE's ``Simulation`` class can be used to generate a synthetic dataset of projection images.\nA ``Simulation`` object produces random projections of a supplied Volume and applies noise and\nCTF filters. The resulting stack of 2D images is stored in an ``Image`` object.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Noise and CTF Filters\nLet's start by creating noise and CTF filters. The ``operators`` package contains a collection\nof filter classes that can be supplied to a ``Simulation``. We use ``ScalarFilter`` to create\nGaussian white noise and ``RadialCTFFilter`` to generate a set of CTF filters with various defocus values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create noise and CTF filters\nfrom aspire.noise import WhiteNoiseAdder\nfrom aspire.operators import RadialCTFFilter\n\n# Gaussian noise filter.\n# Note, the value supplied to the ``WhiteNoiseAdder``, chosen based on other parameters\n# for this quick tutorial, can be changed to adjust the power of the noise.\nnoise_adder = WhiteNoiseAdder(var=1e-5)\n\n# Radial CTF Filter\ndefocus_min = 15000  # unit is angstroms\ndefocus_max = 25000\ndefocus_ct = 7\n\nctf_filters = [\n    RadialCTFFilter(pixel_size=5, defocus=d)\n    for d in np.linspace(defocus_min, defocus_max, defocus_ct)\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Simulation Object\nWe feed our ``Volume`` and filters into ``Simulation`` to generate the dataset of images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from aspire.source import Simulation\n\n# set parameters\nres = 41\nn_imgs = 2500\n\n# For this ``Simulation`` we set all 2D offset vectors to zero,\n# but by default offset vectors will be randomly distributed.\nsrc = Simulation(\n    L=res,  # resolution\n    n=n_imgs,  # number of projections\n    vols=vol,  # volume source\n    offsets=np.zeros((n_imgs, 2)),  # Default: images are randomly shifted\n    noise_adder=noise_adder,\n    unique_filters=ctf_filters,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Several Views of the Projection Images\nWe can access several views of the projection images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# with no corruption applied\nsrc.projections[0:10].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# with no noise corruption\nsrc.clean_images[0:10].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# with noise and CTF corruption\nsrc.images[0:10].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CTF Correction\nWe apply ``phase_flip()`` to correct for CTF effects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "src.phase_flip()\nsrc.images[0:10].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class Averaging\nWe use ``RIRClass2D`` object to classify the images via the rotationally invariant\nrepresentation (RIR) algorithm. Class selection is customizable. The classification module\nalso includes a set of protocols for selecting a set of images to be used for classification.\nHere we're using ``TopClassSelector``, which selects the first ``n_classes`` images from the source.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from aspire.classification import RIRClass2D, TopClassSelector\n\n# set parameters\nn_classes = 200\nn_nbor = 6\n\n# Create a class averaging instance. Note that the ``fspca_components`` and\n# ``bispectrum_components`` were selected for this small tutorial.\nrir = RIRClass2D(\n    src,\n    fspca_components=40,\n    bispectrum_components=30,\n    n_nbor=n_nbor,\n    n_classes=n_classes,\n    selector=TopClassSelector(),\n    num_procs=1,  # Change to \"auto\" if your machine has many processors\n)\n\n# classify and average\nclasses, reflections, distances = rir.classify()\navgs = rir.averages(classes, reflections, distances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View the Class Averages\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Show class averages\navgs.images[0:10].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Show original images corresponding to those classes. This 1:1 comparison is only expected to\n# work because we used ``TopClassSelector`` to classify our images.\nsrc.images[0:10].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Orientation Estimation\nWe initialize a ``CLSyncVoting`` class instance for estimating the orientations of the images.\nThe estimation employs the common lines method with synchronization and voting.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from aspire.abinitio import CLSyncVoting\n\n# Stash true rotations for later comparison\ntrue_rotations = src.rotations[:n_classes]\n\norient_est = CLSyncVoting(avgs, n_theta=72)\n\n# Get the estimated rotations\norient_est.estimate_rotations()\nrots_est = orient_est.rotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mean Squared Error\nASIPRE has some built-in utility functions for globally aligning the estimated rotations\nto the true rotations and computing the mean squared error.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from aspire.utils.coor_trans import (\n    get_aligned_rotations,\n    get_rots_mse,\n    register_rotations,\n)\n\n# Compare with known true rotations\nQ_mat, flag = register_rotations(rots_est, true_rotations)\nregrot = get_aligned_rotations(rots_est, Q_mat, flag)\nmse_reg = get_rots_mse(regrot, true_rotations)\nmse_reg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Volume Reconstruction\nNow that we have our class averages and rotation estimates, we can estimate the\nmean volume by supplying the class averages and basis for back projection.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from aspire.basis import FFBBasis3D\nfrom aspire.reconstruction import MeanEstimator\n\n# Assign the estimated rotations to the class averages\navgs.rotations = rots_est\n\n# Create a reasonable Basis for the 3d Volume\nbasis = FFBBasis3D(res, dtype=vol.dtype)\n\n# Setup an estimator to perform the back projection.\nestimator = MeanEstimator(avgs, basis)\n\n# Perform the estimation and save the volume.\nestimated_volume = estimator.estimate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of Estimated Volume with Source Volume\nTo get a visual confirmation that our results are sane, we rotate the\nestimated volume by the estimated rotations and project along the z-axis.\nThese estimated projections should align with the original projection images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from aspire.source import ArrayImageSource\n\n# Get projections from the estimated volume using the estimated orientations.\n# We instantiate the projections as an ``ArrayImageSource`` to access the ``Image.show()`` method.\nprojections_est = ArrayImageSource(estimated_volume.project(0, rots_est))\n\n# We view the first 10 projections of the estimated volume.\nprojections_est.images[0:10].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For comparison, we view the first 10 source projections.\nsrc.projections[0:10].show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}